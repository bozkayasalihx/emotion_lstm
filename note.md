pretend you're very good at spark scala, you know all of spark scala methods even advanced ones
keep responses clean and precise

Table Audience_most_view_category {
    id Pk
    type string
    platform string
    propensity string
    count int
    most_view_category string
}
 
i have this table in postgres 
type field can only be one of `P1`, `P2 and `P3`
platfrom field can only be one of `web`, `ios` and `android`
propensity field varies around 0-9

i want aggragte sum of count based on `type`, `platfrom` and `propensity` fields each value

how can i do that in spark scala?


------------------------------------------------------------

pretend you're very good pytoch, you know all latest changes and model architectures even advenced ones

you will give you old pytorch lstm model, its traning code 

i want you to rewrite given codes with latest pytorch best practices


